{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "min_word_freq = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/Ravikumar/Developer/chatbot/dataset/movie_conversations.txt\",\"r\") as c_file:\n",
    "    conversations = c_file.readlines()\n",
    "\n",
    "with open(\"/home/Ravikumar/Developer/chatbot/dataset/movie_lines.txt\",\"r\",encoding=\"iso-8859-1\") as l_file:\n",
    "    lines = l_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict = dict()\n",
    "for line in lines:\n",
    "    objects =  line.split(' +++$+++ ')\n",
    "    lines_dict[objects[0]] = objects[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def remove_punc(string):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char  # space is also a character\n",
    "    return no_punct.lower()\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list()\n",
    "for conv in conversations:\n",
    "    ids = eval(conv.split(\"+++$+++\")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = list()\n",
    "        if i == len(ids)-1:\n",
    "            break\n",
    "        first = normalizeString(remove_punc(unicodeToAscii(lines_dict[ids[i]].strip())))\n",
    "        second = remove_punc(lines_dict[ids[i+1]].strip())\n",
    "        qa_pairs.append(first.split()[:MAX_LEN])\n",
    "        qa_pairs.append(second.split()[:MAX_LEN])\n",
    "        pairs.append(qa_pairs)\n",
    "        \n",
    "    # for i in range(len(ids)):\n",
    "        # print(lines_dict[ids[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k:v+1 for v,k in enumerate(words)}\n",
    "word_map[\"<unk>\"] = len(word_map)+1\n",
    "word_map[\"<start>\"] = len(word_map)+1\n",
    "word_map[\"<end>\"] = len(word_map)+1\n",
    "word_map[\"<pad>\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are : 14117\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total words are : {len(word_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/Ravikumar/Developer/chatbot/dataset/vocab.json\",\"w\") as v:\n",
    "    json.dump(word_map,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(words,word_map):\n",
    "    enc = [word_map.get(word,word_map[\"<unk>\"]) for word in words] +[word_map[\"<end>\"]]+ [word_map[\"<pad>\"]]* (MAX_LEN - len(words))\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_reply(words,word_map):\n",
    "    enc_c = [word_map[\"<start>\"]] +  [word_map.get(word, word_map['<unk>']) for word in words] +[word_map[\"<pad>\"]] * (MAX_LEN - len(words))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    qus = encode_question(pair[0], word_map)\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    pairs_encoded.append((qus, ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 14114, 14114, 6, 7, 8, 14116],\n",
       " [14115, 9, 10, 11, 12, 13, 14, 14114, 15, 16, 17])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k,v in word_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'we', 'make', 'this', 'quick', '<unk>', '<unk>', 'and', 'andrew', 'barrett', '<end>']\n",
      "['<start>', 'well', 'i', 'thought', 'wed', 'start', 'with', '<unk>', 'if', 'thats', 'okay']\n"
     ]
    }
   ],
   "source": [
    "print([index2word[idx] for idx in pairs_encoded[0][0]])\n",
    "print([index2word[idx] for idx in pairs_encoded[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/Ravikumar/Developer/chatbot/dataset/pairs_encoded.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence size of input(question) : 11\n",
      "Sequence size of output(reply)   : 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequence size of input(question) :\",len(pairs_encoded[100][0]))\n",
    "print(\"Sequence size of output(reply)   :\",len(pairs_encoded[100][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples :  221616\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of samples : \",len(pairs_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(seq):\n",
    "    mask = list()\n",
    "    for s in seq:\n",
    "        if s == 0:\n",
    "            mask.append(False)\n",
    "        else:\n",
    "            mask.append(True)\n",
    "    return torch.tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence : [110, 92, 161, 14116, 0, 0, 0, 0, 0, 0, 0]  #input_size : 11\n",
      "output sequence : [14115, 10, 162, 2, 163, 164, 165, 14114, 0, 0, 0] #input_size : 11\n",
      "Mask :  tensor([ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "        False])\n"
     ]
    }
   ],
   "source": [
    "# 25th example to show the mask and input sequence \n",
    "ins = pairs_encoded[25][0]\n",
    "outs = pairs_encoded[25][1]\n",
    "\n",
    "print(f\"input sequence : {ins}  #input_size : {len(ins)}\")\n",
    "print(f\"output sequence : {outs} #input_size : {len(outs)}\")\n",
    "print(\"Mask : \",masking(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14117"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95aac795b256ca81790b03f5327d66a3955fee3b81dfc95d5d267fe1f88b1813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
